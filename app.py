import streamlit as st
import os
import docx
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import google.generativeai as genai
import time

# --- 1. å¾ Streamlit Secrets è®€å– API Key ---
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
else:
    st.error("âŒ éŒ¯èª¤ï¼šæœªåœ¨ Streamlit Secrets ä¸­è¨­å®š GOOGLE_API_KEYã€‚")
    st.stop()

EMBEDDING_MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'

# --- æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

@st.cache_resource
def load_embedding_model():
    return SentenceTransformer(EMBEDDING_MODEL_NAME)

@st.cache_resource
def create_faiss_index(_embeddings):
    d = _embeddings.shape[1]
    index = faiss.IndexFlatL2(d)
    index.add(_embeddings)
    return index

def load_documents(folder_path):
    doc_texts, doc_names = [], []
    # åƒ…è®€å– .docx
    files = [f for f in os.listdir(folder_path) if f.endswith('.docx') and not f.startswith('~$')]
    for filename in files:
        try:
            full_path = os.path.join(folder_path, filename)
            doc = docx.Document(full_path)
            full_text = "\n\n".join([para.text for para in doc.paragraphs if para.text.strip()])
            doc_texts.append(full_text)
            doc_names.append(filename)
        except Exception as e:
            st.error(f"è®€å– {filename} å‡ºéŒ¯: {e}")
    return doc_names, doc_texts

def generate_answer(query, context):
    """
    è‡ªå‹•åˆ‡æ›æ¨¡å‹é‚è¼¯ï¼šè§£æ±º 404 æ‰¾ä¸åˆ°æ¨¡å‹æˆ– 429 é…é¡æ»¿çš„å•é¡Œ
    """
    # æ ¹æ“šæ‚¨çš„è¨ºæ–·æ¸…å–®ï¼Œè¨­å®šå„ªå…ˆé †åº
    model_candidates = [
        "models/gemini-2.0-flash", 
        "models/gemini-flash-latest",
        "models/gemini-1.5-flash"
    ]
    
    prompt = f"ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ”¾é›»æ©ŸåŠ©æ‰‹ã€‚è«‹æ ¹æ“šæ‰‹å†Šå›ç­”å•é¡Œï¼š\n\n{context}\n\nå•é¡Œï¼š{query}\nå›ç­”ï¼ˆç¹é«”ä¸­æ–‡ï¼‰ï¼š"

    for model_name in model_candidates:
        try:
            model = genai.GenerativeModel(model_name)
            response = model.generate_content(prompt)
            return response.text, model_name
        except Exception as e:
            err_msg = str(e)
            # å¦‚æœæ˜¯ 404 (æ‰¾ä¸åˆ°æ¨¡å‹) æˆ– 429 (é…é¡æ»¿)ï¼Œå‰‡å˜—è©¦æ¸…å–®ä¸­çš„ä¸‹ä¸€å€‹æ¨¡å‹
            if "404" in err_msg or "429" in err_msg:
                continue
            else:
                return f"ç”¢ç”Ÿç­”æ¡ˆæ™‚ç™¼ç”Ÿéé æœŸéŒ¯èª¤ï¼š{err_msg}", "Error"
                
    return "æ‰€æœ‰å¯ç”¨æ¨¡å‹å‡ç„¡æ³•æœå‹™ (404 æˆ– 429)ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚", "None"

# --- åˆå§‹åŒ– ---
st.set_page_config(page_title="æ”¾é›»æ©Ÿ AI åŠ©ç†", page_icon="âš¡")
st.title("âš¡ æ”¾é›»æ©Ÿæ“ä½œ AI å°å¹«æ‰‹")

if 'initialized' not in st.session_state:
    with st.spinner("ç³»çµ±åˆ†ææ–‡ä»¶ä¸­..."):
        st.session_state.model = load_embedding_model()
        current_folder = os.path.dirname(os.path.abspath(__file__))
        doc_names, doc_texts = load_documents(current_folder)
        
        if not doc_texts:
            st.error("æ‰¾ä¸åˆ°å¯è®€å–çš„ .docx æª”æ¡ˆï¼è«‹ç¢ºèªæª”æ¡ˆæ ¼å¼æ­£ç¢ºã€‚")
            st.stop()
            
        paragraphs = []
        for text in doc_texts:
            paragraphs.extend([p.strip() for p in text.split('\n\n') if p.strip()])
        
        embeddings = st.session_state.model.encode(paragraphs)
        st.session_state.chunks = paragraphs
        st.session_state.faiss_index = create_faiss_index(np.array(embeddings))
        st.session_state.initialized = True
        st.success(f"âœ… æˆåŠŸè¼‰å…¥ {len(doc_names)} ä»½æ‰‹å†Šã€‚")

# --- UI ---
query = st.text_input("è«‹è¼¸å…¥æ“ä½œå•é¡Œï¼š", placeholder="ä¾‹å¦‚ï¼šå¦‚ä½•è¨­å®šæ¥µé–“é›»å£“ï¼Ÿ")

if st.button("è©¢å• AI"):
    if query:
        with st.spinner("æœå°‹ç­”æ¡ˆä¸­..."):
            query_embedding = st.session_state.model.encode([query])
            _, indices = st.session_state.faiss_index.search(query_embedding, 5)
            context = "\n\n".join([st.session_state.chunks[i] for i in indices[0]])
            
            answer, used_model = generate_answer(query, context)
            st.markdown(f"### ğŸ¤– AI å›ç­” (ä½¿ç”¨æ¨¡å‹: {used_model})")
            st.info(answer)
    else:
        st.warning("è«‹è¼¸å…¥å•é¡Œå…§å®¹ã€‚")