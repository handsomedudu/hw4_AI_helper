import streamlit as st
import os
import docx
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import google.generativeai as genai

# ==========================================
# 1. é€™è£¡ç›´æ¥å¯«æ­»æ‚¨çš„ API KEY
# ==========================================
MY_API_KEY = "AIzaSyBVF_HR40eAuH_MmevkgWe5E33Ielm0eCw" 

# è¨­å®š Google Gemini é…ç½®
genai.configure(api_key=MY_API_KEY)

# å‘é‡æ¨¡å‹åç¨± (å¤šåœ‹èªè¨€ç‰ˆ)
EMBEDDING_MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'

# --- æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

@st.cache_resource
def load_embedding_model():
    """è¼‰å…¥èªæ„åˆ†ææ¨¡å‹"""
    with st.spinner("æ­£åœ¨è¼‰å…¥AIæ¨¡å‹..."):
        model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    return model

@st.cache_resource
def create_faiss_index(_embeddings):
    """å»ºç«‹ FAISS é«˜é€Ÿæª¢ç´¢ç´¢å¼•"""
    d = _embeddings.shape[1]
    index = faiss.IndexFlatL2(d)
    index.add(_embeddings)
    return index

def load_documents(folder_path):
    """è®€å–ç•¶å‰ç›®éŒ„ä¸‹çš„ .docx æ‰‹å†Šæª”æ¡ˆ"""
    doc_texts, doc_names = [], []
    # ä¿®æ­£ï¼šåƒ…ç¯©é¸ .docx æª”æ¡ˆï¼Œé¿é–‹ç„¡æ³•è®€å–çš„ .doc
    files = [f for f in os.listdir(folder_path) if f.endswith('.docx') and not f.startswith('~$')]
    
    for filename in files:
        try:
            full_path = os.path.join(folder_path, filename)
            doc = docx.Document(full_path)
            # æå–æ‰€æœ‰æ®µè½æ–‡å­—
            full_text = "\n\n".join([para.text for para in doc.paragraphs if para.text.strip()])
            doc_texts.append(full_text)
            doc_names.append(filename)
        except Exception as e:
            st.error(f"è®€å– {filename} å‡ºéŒ¯: {e}")
            
    return doc_names, doc_texts

def split_text(doc_names, doc_texts):
    """å°‡é•·æ–‡æœ¬åˆ‡å‰²æˆæ®µè½ (Chunks)"""
    chunks, chunk_sources = [], []
    for i, text in enumerate(doc_texts):
        # ä¾æ®µè½åˆ‡åˆ†ï¼Œéæ¿¾ç©ºè¡Œ
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        chunks.extend(paragraphs)
        chunk_sources.extend([doc_names[i]] * len(paragraphs))
    return chunks, chunk_sources

def generate_answer(query, context):
    """èª¿ç”¨æœ€æ–°çš„ Gemini 1.5 Flash ç”Ÿæˆç­”æ¡ˆ"""
    prompt = f"""
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ”¾é›»æ©Ÿ (EDM) æ“ä½œåŠ©æ‰‹ã€‚
    è«‹åƒ…æ ¹æ“šä»¥ä¸‹æ‰‹å†Šå…§å®¹å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚å¦‚æœæ‰‹å†Šå…§å®¹ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹èªªä¸çŸ¥é“ã€‚
    è«‹å›ç­”å¾—è©³ç´°ä¸”å°ˆæ¥­ã€‚
    
    --- æ‰‹å†Šå…§å®¹ (CONTEXT) ---
    {context}
    ---
    
    ä½¿ç”¨è€…çš„å•é¡Œï¼š{query}

    å›ç­”ï¼ˆç¹é«”ä¸­æ–‡ï¼‰ï¼š
    """
    try:
        # ä½¿ç”¨ models/ å‰ç¶´ç¢ºä¿è·¯å¾‘æ­£ç¢ºï¼Œä¸¦æ”¹ç”¨ 1.5-flash
        model = genai.GenerativeModel("models/gemini-1.5-flash")
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Gemini API ç”¢ç”Ÿç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"

# --- åˆå§‹åŒ–æµç¨‹ ---

# è¨­å®šé é¢è³‡è¨Š
st.set_page_config(page_title="æ”¾é›»æ©Ÿ AI åŠ©ç†", page_icon="âš¡")
st.title("âš¡ æ”¾é›»æ©Ÿæ“ä½œ AI å°å¹«æ‰‹")
st.caption("æœ¬ç³»çµ±å·²å…§å»º AI æˆæ¬Šï¼Œä¸¦è‡ªå‹•è®€å–ç›®éŒ„ä¸‹çš„ Word æ“ä½œæ‰‹å†Šã€‚")

if 'initialized' not in st.session_state:
    with st.spinner("ç³»çµ±æ­£åœ¨åˆå§‹åŒ–æ–‡ä»¶èˆ‡å‘é‡è³‡æ–™åº«ï¼Œè«‹ç¨å€™..."):
        # 1. è¼‰å…¥æ¨¡å‹
        st.session_state.model = load_embedding_model()
        
        # 2. è®€å–æ–‡ä»¶ (app.py æ‰€åœ¨ç›®éŒ„)
        current_folder = os.path.dirname(os.path.abspath(__file__))
        doc_names, doc_texts = load_documents(current_folder)
        
        if not doc_texts:
            st.error("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ°ä»»ä½•å¯è®€å–çš„ .docx æª”æ¡ˆï¼è«‹ç¢ºèªæ‰‹å†Šå·²ä¸Šå‚³ã€‚")
            st.stop()
            
        # 3. å»ºç«‹å‘é‡è³‡æ–™åº« (RAG æ ¸å¿ƒ)
        chunks, _ = split_text(doc_names, doc_texts)
        embeddings = st.session_state.model.encode(chunks, show_progress_bar=False)
        st.session_state.chunks = chunks
        st.session_state.faiss_index = create_faiss_index(np.array(embeddings))
        
        st.session_state.initialized = True
        st.success(f"âœ… æˆåŠŸåˆå§‹åŒ–ï¼å·²è¼‰å…¥ {len(doc_names)} ä»½æ‰‹å†Šã€‚")

# --- ä½¿ç”¨è€…å°è©±ä»‹é¢ ---

st.divider()
query = st.text_input("è«‹è¼¸å…¥æ‚¨çš„æ“ä½œå•é¡Œï¼š", placeholder="ä¾‹å¦‚ï¼šå¦‚ä½•è¨­å®šæ¥µé–“é›»å£“ï¼Ÿ")

if st.button("è©¢å• AI å°å¹«æ‰‹"):
    if query:
        with st.spinner("æ­£åœ¨æœå°‹æ‰‹å†Šä¸¦åˆ†æç­”æ¡ˆ..."):
            # A. å‘é‡æª¢ç´¢ (æ‰¾å‡ºæœ€ç›¸é—œçš„ 5 å€‹ç‰‡æ®µ)
            query_embedding = st.session_state.model.encode([query])
            distances, indices = st.session_state.faiss_index.search(query_embedding, 5)
            
            # çµ„åˆä¸Šä¸‹æ–‡
            retrieved_chunks = [st.session_state.chunks[i] for i in indices[0] if i != -1]
            context = "\n\n".join(retrieved_chunks)
            
            # B. ç”Ÿæˆç­”æ¡ˆ
            answer = generate_answer(query, context)
            
            # C. é¡¯ç¤ºçµæœ
            st.markdown("### ğŸ¤– AI çš„å›ç­”ï¼š")
            st.success(answer)
            
            # D. æä¾›ä¾†æºæŸ¥é–± (å¢åŠ é€æ˜åº¦)
            with st.expander("ğŸ” æŸ¥çœ‹åƒè€ƒçš„æ‰‹å†Šä¾†æºæ®µè½"):
                for idx, chunk in enumerate(retrieved_chunks):
                    st.info(f"ä¾†æºç‰‡æ®µ {idx+1}:\n{chunk}")
    else:
        st.warning("è«‹å…ˆè¼¸å…¥æ‚¨çš„å•é¡Œã€‚")