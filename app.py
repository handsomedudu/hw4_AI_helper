import streamlit as st
import os
import docx
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import google.generativeai as genai

# --- Constants ---
# ä½¿ç”¨å¤šåœ‹èªè¨€å‘é‡æ¨¡å‹ï¼Œé©åˆè™•ç†ç¹é«”ä¸­æ–‡æ‰‹å†Š
EMBEDDING_MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'

# --- Functions ---

@st.cache_resource
def load_embedding_model():
    """è¼‰å…¥å‘é‡æ¨¡å‹ (SentenceTransformer)"""
    with st.spinner("æ­£åœ¨è¼‰å…¥èªæ„åˆ†ææ¨¡å‹..."):
        model = SentenceTransformer(EMBEDDING_MODEL_NAME)
    return model

@st.cache_resource
def create_faiss_index(_embeddings):
    """å»ºç«‹ FAISS é«˜é€Ÿæª¢ç´¢ç´¢å¼•"""
    d = _embeddings.shape[1]
    index = faiss.IndexFlatL2(d)
    index.add(_embeddings)
    return index

def load_documents(folder_path):
    """è®€å–è³‡æ–™å¤¾å…§çš„ .doc å’Œ .docx æª”æ¡ˆ"""
    doc_texts, doc_names = [], []
    # éæ¿¾æš«å­˜æª”ä¸¦è®€å– Word æª”æ¡ˆ
    files = [f for f in os.listdir(folder_path) if (f.endswith('.docx') or f.endswith('.doc')) and not f.startswith('~$')]
    for filename in files:
        try:
            full_path = os.path.join(folder_path, filename)
            doc = docx.Document(full_path)
            full_text = "\n\n".join([para.text for para in doc.paragraphs if para.text.strip()])
            doc_texts.append(full_text)
            doc_names.append(filename)
        except Exception as e:
            st.error(f"è®€å–æª”æ¡ˆ {filename} æ™‚å‡ºéŒ¯: {e}")
    return doc_names, doc_texts

def split_text(doc_names, doc_texts):
    """å°‡æ–‡ä»¶åˆ‡å‰²æˆé©åˆ AI é–±è®€çš„æ®µè½ (Chunks)"""
    chunks, chunk_sources = [], []
    for i, text in enumerate(doc_texts):
        # ä¾æ®µè½åˆ‡åˆ†ï¼Œéæ¿¾ç©ºè¡Œ
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        chunks.extend(paragraphs)
        chunk_sources.extend([doc_names[i]] * len(paragraphs))
    return chunks, chunk_sources

def search_index(query, model, index, chunks, k=5):
    """åœ¨å‘é‡ç´¢å¼•ä¸­æœå°‹èˆ‡å•é¡Œæœ€ç›¸é—œçš„ 5 å€‹æ®µè½"""
    query_embedding = model.encode([query])
    distances, indices = index.search(query_embedding, k)
    
    # å–å¾—ä¸é‡è¤‡çš„ç´¢å¼•ç·¨è™Ÿ
    unique_indices = list(dict.fromkeys(indices[0]))
    results = [chunks[i] for i in unique_indices if i != -1]
    return results

def generate_answer(query, context):
    """èª¿ç”¨ Gemini API ç”Ÿæˆå›ç­”"""
    prompt = f"""
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ”¾é›»æ©Ÿ (EDM) æ“ä½œåŠ©æ‰‹ã€‚
    è«‹åƒ…æ ¹æ“šä»¥ä¸‹æä¾›çš„æ“ä½œæ‰‹å†Šå…§å®¹ä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
    å¦‚æœæ‰‹å†Šå…§å®¹ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹ç¦®è²Œåœ°å‘ŠçŸ¥ä½ ä¸çŸ¥é“ï¼Œä¸è¦è‡ªè¡Œç·¨é€ ã€‚
    
    --- æ‰‹å†Šå…§å®¹ (CONTEXT) ---
    {context}
    --- çµæŸå…§å®¹ ---

    ä½¿ç”¨è€…çš„å•é¡Œï¼š{query}

    è«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼š
    """
    try:
        # ä½¿ç”¨æœ€æ–°çš„ 1.5 flash æ¨¡å‹ï¼Œé€Ÿåº¦å¿«ä¸”æ”¯æ´é•·æ–‡æœ¬
        model = genai.GenerativeModel("gemini-1.5-flash")
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Gemini API ç”¢ç”Ÿç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"

# --- Streamlit ä»‹é¢ä½ˆå±€ ---

st.set_page_config(page_title="EDM AI Assistant", layout="wide")
st.title("âš¡ æ”¾é›»æ©Ÿæ“ä½œæ‰‹å†Šå•ç­”å°å¹«æ‰‹")
st.caption("åŸºæ–¼ Generative AI (RAG) æŠ€è¡“çš„å·¥æ¥­æ“ä½œè¼”åŠ©ç³»çµ±")

# å´é‚Šæ¬„è¨­å®š
with st.sidebar:
    st.header("è¨­å®š (Settings)")
    # ä¿®æ­£ï¼šå®šç¾© google_api_key
    google_api_key = st.text_input("è¼¸å…¥ Google API Key", type="password")
    st.markdown("[å¦‚ä½•å–å¾— API Key?](https://aistudio.google.com/app/apikey)")
    st.divider()
    st.info("è«‹å°‡ Word æ‰‹å†Šæª”æ¡ˆæ”¾åœ¨èˆ‡ app.py ç›¸åŒçš„ç›®éŒ„ä¸‹ã€‚")

# --- åˆå§‹åŒ–èˆ‡è³‡æ–™è™•ç†é‚è¼¯ ---

def initialize(api_key):
    """å•Ÿå‹•æ™‚çš„åˆå§‹åŒ–æµç¨‹"""
    genai.configure(api_key=api_key)
    
    # è¼‰å…¥æ¨¡å‹
    st.session_state.model = load_embedding_model()

    # è®€å–ç•¶å‰ç›®éŒ„ä¸‹çš„æ–‡ä»¶
    current_folder = os.path.dirname(os.path.abspath(__file__))
    doc_names, doc_texts = load_documents(current_folder)
    
    if not doc_texts:
        st.error("æ‰¾ä¸åˆ°æ‰‹å†Šæª”æ¡ˆï¼è«‹ç¢ºä¿ .docx æª”æ¡ˆå·²ä¸Šå‚³ã€‚")
        st.stop()
        
    with st.spinner("æ­£åœ¨å»ºç«‹èªæ„ç´¢å¼•åº«..."):
        chunks, chunk_sources = split_text(doc_names, doc_texts)
        embeddings = st.session_state.model.encode(chunks, show_progress_bar=True)
        
        st.session_state.chunks = chunks
        st.session_state.faiss_index = create_faiss_index(np.array(embeddings))
        st.session_state.initialized = True
    
    st.success(f"âœ… æˆåŠŸè¼‰å…¥ {len(doc_names)} ä»½æ‰‹å†Šï¼Œå…± {len(chunks)} å€‹æ®µè½ã€‚")

# æª¢æŸ¥ API Key ä¸¦åŸ·è¡Œåˆå§‹åŒ–
if google_api_key:
    if 'initialized' not in st.session_state:
        initialize(google_api_key)
else:
    st.warning("è«‹å…ˆåœ¨å·¦å´è¼¸å…¥ Google API Key ä»¥å•Ÿå‹•ç³»çµ±ã€‚")
    st.stop()

# --- ä¸»è¦å•ç­”ä»‹é¢ ---

query = st.text_input("è«‹è¼¸å…¥é—œæ–¼æ”¾é›»æ©Ÿæ“ä½œçš„å•é¡Œ (ä¾‹å¦‚ï¼šå¦‚ä½•è¨­å®šæ¥µé–“é›»å£“ï¼Ÿ)", "")

if st.button("é–‹å§‹è©¢å•"):
    if query:
        with st.spinner("æœå°‹æ‰‹å†Šä¸­..."):
            # 1. æª¢ç´¢ç›¸é—œå…§å®¹
            retrieved_chunks = search_index(
                query, 
                st.session_state.model, 
                st.session_state.faiss_index, 
                st.session_state.chunks
            )
            context_text = "\n\n".join(retrieved_chunks)

            # 2. ç”Ÿæˆå›ç­”
            answer = generate_answer(query, context_text)
            
            # 3. é¡¯ç¤ºçµæœ
            st.markdown("### ğŸ¤– AI çš„å›ç­”ï¼š")
            st.write(answer)
            
            with st.expander("ğŸ” æŸ¥çœ‹åƒè€ƒä¾†æº"):
                for i, text in enumerate(retrieved_chunks):
                    st.info(f"ä¾†æºç‰‡æ®µ {i+1}:\n{text}")
    else:
        st.warning("è«‹è¼¸å…¥å•é¡Œå…§å®¹ã€‚")