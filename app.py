import streamlit as st
import os
import docx
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import google.generativeai as genai

# ==========================================
# 1. API KEY å¯«æ­»è¨­å®š (è«‹ç¢ºèªæ­¤ Key æœ‰æ•ˆ)
# ==========================================
MY_API_KEY = "AIzaSyBVF_HR40eAuH_MmevkgWe5E33Ielm0eCw" 
genai.configure(api_key=MY_API_KEY)

# å‘é‡æ¨¡å‹åç¨± (å¤šåœ‹èªè¨€ç‰ˆ)
EMBEDDING_MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'

# --- æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

@st.cache_resource
def load_embedding_model():
    """è¼‰å…¥èªæ„åˆ†ææ¨¡å‹"""
    return SentenceTransformer(EMBEDDING_MODEL_NAME)

@st.cache_resource
def create_faiss_index(_embeddings):
    """å»ºç«‹ FAISS é«˜é€Ÿæª¢ç´¢ç´¢å¼•"""
    d = _embeddings.shape[1]
    index = faiss.IndexFlatL2(d)
    index.add(_embeddings)
    return index

def load_documents(folder_path):
    """è®€å–ç•¶å‰ç›®éŒ„ä¸‹çš„ .docx æ‰‹å†Šæª”æ¡ˆ"""
    doc_texts, doc_names = [], []
    # åƒ…ç¯©é¸ .docx æª”æ¡ˆï¼Œé¿é–‹ç„¡æ³•è®€å–çš„èˆŠç‰ˆ .doc
    files = [f for f in os.listdir(folder_path) if f.endswith('.docx') and not f.startswith('~$')]
    
    for filename in files:
        try:
            full_path = os.path.join(folder_path, filename)
            doc = docx.Document(full_path)
            full_text = "\n\n".join([para.text for para in doc.paragraphs if para.text.strip()])
            doc_texts.append(full_text)
            doc_names.append(filename)
        except Exception as e:
            st.error(f"è®€å– {filename} å‡ºéŒ¯ (è«‹ç¢ºèªæ˜¯å¦ç‚º .docx æ ¼å¼): {e}")
            
    return doc_names, doc_texts

def split_text(doc_names, doc_texts):
    """å°‡æ–‡æœ¬åˆ‡å‰²æˆæ®µè½ (Chunks)"""
    chunks, chunk_sources = [], []
    for i, text in enumerate(doc_texts):
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        chunks.extend(paragraphs)
        chunk_sources.extend([doc_names[i]] * len(paragraphs))
    return chunks, chunk_sources

def get_best_model_name():
    """æ ¹æ“šæ‚¨çš„è¨ºæ–·æ¸…å–®ï¼Œè‡ªå‹•é¸å–æœ€åˆé©çš„æ¨¡å‹"""
    try:
        available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
        # å„ªå…ˆé †åºï¼š2.0-flash > flash-latest > 1.5-flash
        priority_list = [
            "models/gemini-2.0-flash", 
            "models/gemini-flash-latest", 
            "models/gemini-1.5-flash"
        ]
        for target in priority_list:
            if target in available_models:
                return target
        return available_models[0] if available_models else "models/gemini-2.0-flash"
    except:
        return "models/gemini-2.0-flash"

def generate_answer(query, context):
    """èª¿ç”¨ Gemini ç”Ÿæˆç­”æ¡ˆ"""
    target_model = get_best_model_name()
    prompt = f"""
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ”¾é›»æ©Ÿ (EDM) æ“ä½œåŠ©æ‰‹ã€‚
    è«‹åƒ…æ ¹æ“šä»¥ä¸‹æ‰‹å†Šå…§å®¹å›ç­”å•é¡Œã€‚å¦‚æœæ‰‹å†Šå…§å®¹ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹èªªä¸çŸ¥é“ã€‚
    
    --- æ‰‹å†Šå…§å®¹ (CONTEXT) ---
    {context}
    ---
    
    å•é¡Œï¼š{query}

    å›ç­”ï¼ˆç¹é«”ä¸­æ–‡ï¼‰ï¼š
    """
    try:
        model = genai.GenerativeModel(target_model)
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"ç”¢ç”Ÿç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}\n(å˜—è©¦æ¨¡å‹: {target_model})"

# --- åˆå§‹åŒ–æµç¨‹ ---

st.set_page_config(page_title="æ”¾é›»æ©Ÿ AI åŠ©ç†", page_icon="âš¡")
st.title("âš¡ æ”¾é›»æ©Ÿæ“ä½œ AI å°å¹«æ‰‹")
st.caption("åŸºæ–¼ RAG æŠ€è¡“èˆ‡ Gemini 2.0 çš„å°ˆæ¥­å•ç­”ç³»çµ±")

if 'initialized' not in st.session_state:
    with st.spinner("ç³»çµ±åˆå§‹åŒ–ä¸­..."):
        st.session_state.model = load_embedding_model()
        current_folder = os.path.dirname(os.path.abspath(__file__))
        doc_names, doc_texts = load_documents(current_folder)
        
        if not doc_texts:
            st.error("æ‰¾ä¸åˆ° .docx æª”æ¡ˆï¼è«‹ç¢ºèªå·²å°‡æ‰‹å†Šä¸Šå‚³è‡³ç›®éŒ„ã€‚")
            st.stop()
            
        chunks, _ = split_text(doc_names, doc_texts)
        embeddings = st.session_state.model.encode(chunks)
        st.session_state.chunks = chunks
        st.session_state.faiss_index = create_faiss_index(np.array(embeddings))
        st.session_state.initialized = True
        st.success(f"âœ… å·²æˆåŠŸåˆ†æ {len(st.session_state.chunks)} å€‹æ®µè½ã€‚")

# --- UI ä»‹é¢ ---

query = st.text_input("è«‹è¼¸å…¥æ“ä½œå•é¡Œï¼š", placeholder="ä¾‹å¦‚ï¼šå¦‚ä½•é€²è¡Œå·¥ä»¶å°‹é‚Šï¼Ÿ")

if st.button("è©¢å• AI"):
    if query:
        with st.spinner("æœå°‹æ‰‹å†Šä¸­..."):
            query_embedding = st.session_state.model.encode([query])
            _, indices = st.session_state.faiss_index.search(query_embedding, 5)
            context = "\n\n".join([st.session_state.chunks[i] for i in indices[0] if i != -1])
            
            answer = generate_answer(query, context)
            
            st.markdown("### ğŸ¤– AI å›ç­”ï¼š")
            st.success(answer)
            
            with st.expander("ğŸ” æŸ¥çœ‹åƒè€ƒä¾†æºæ®µè½"):
                st.write(context)
    else:
        st.warning("è«‹è¼¸å…¥å•é¡Œå…§å®¹ã€‚")

# --- è¨ºæ–·è³‡è¨Š ---
with st.expander("ğŸ› ï¸ ç³»çµ±ç‹€æ…‹è¨ºæ–·"):
    st.write(f"ç•¶å‰è‡ªå‹•é¸ç”¨æ¨¡å‹: `{get_best_model_name()}`")
    st.write(f"æ‰‹å†Šæ®µè½ç¸½æ•¸: {len(st.session_state.chunks)}")