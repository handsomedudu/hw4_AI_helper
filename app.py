import streamlit as st
import os
import docx
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import google.generativeai as genai

# --- 1. å¾ Streamlit Secrets å®‰å…¨è®€å– API Key ---
if "GOOGLE_API_KEY" in st.secrets:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
else:
    st.error("âŒ éŒ¯èª¤ï¼šæœªåœ¨ Streamlit Cloud çš„ Secrets ä¸­æ‰¾åˆ° GOOGLE_API_KEYã€‚è«‹å…ˆå®Œæˆè¨­å®šã€‚")
    st.stop()

# å‘é‡æ¨¡å‹åç¨± (é©åˆç¹é«”ä¸­æ–‡æ‰‹å†Š)
EMBEDDING_MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'

# --- æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

@st.cache_resource
def load_embedding_model():
    """è¼‰å…¥èªæ„åˆ†ææ¨¡å‹"""
    return SentenceTransformer(EMBEDDING_MODEL_NAME)

@st.cache_resource
def create_faiss_index(_embeddings):
    """å»ºç«‹ FAISS é«˜é€Ÿæª¢ç´¢ç´¢å¼•"""
    d = _embeddings.shape[1]
    index = faiss.IndexFlatL2(d)
    index.add(_embeddings)
    return index

def load_documents(folder_path):
    """è®€å–ç•¶å‰ç›®éŒ„ä¸‹çš„ .docx æ‰‹å†Šæª”æ¡ˆ"""
    doc_texts, doc_names = [], []
    # åƒ…ç¯©é¸ .docx æª”æ¡ˆï¼Œé¿å… python-docx ç„¡æ³•è®€å–èˆŠç‰ˆ .doc å°è‡´å´©æ½°
    files = [f for f in os.listdir(folder_path) if f.endswith('.docx') and not f.startswith('~$')]
    
    for filename in files:
        try:
            full_path = os.path.join(folder_path, filename)
            doc = docx.Document(full_path)
            full_text = "\n\n".join([para.text for para in doc.paragraphs if para.text.strip()])
            doc_texts.append(full_text)
            doc_names.append(filename)
        except Exception as e:
            st.error(f"è®€å– {filename} å‡ºéŒ¯: {e}")
            
    return doc_names, doc_texts

def split_text(doc_names, doc_texts):
    """å°‡é•·æ–‡æœ¬åˆ‡å‰²æˆæ®µè½ (Chunks)"""
    chunks, chunk_sources = [], []
    for i, text in enumerate(doc_texts):
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        chunks.extend(paragraphs)
        chunk_sources.extend([doc_names[i]] * len(paragraphs))
    return chunks, chunk_sources

def generate_answer(query, context):
    """èª¿ç”¨ Gemini 2.0 Flash ç”Ÿæˆç­”æ¡ˆ"""
    # æ ¹æ“šåµæ¸¬æ¸…å–®ï¼Œmodels/gemini-2.0-flash æ˜¯æ‚¨å¸³è™Ÿç›®å‰æœ€ç©©å®šä¸”é«˜æ•ˆçš„é¸æ“‡
    target_model = "models/gemini-2.0-flash"
    
    prompt = f"""
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ”¾é›»æ©Ÿ (EDM) æ“ä½œåŠ©æ‰‹ã€‚
    è«‹åƒ…æ ¹æ“šä»¥ä¸‹æ‰‹å†Šå…§å®¹å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚å¦‚æœæ‰‹å†Šå…§å®¹ä¸­æ²’æœ‰ç­”æ¡ˆï¼Œè«‹ç¦®è²Œåœ°å‘ŠçŸ¥ä½ ä¸çŸ¥é“ã€‚
    
    --- æ‰‹å†Šå…§å®¹ (CONTEXT) ---
    {context}
    ---
    
    å•é¡Œï¼š{query}

    å›ç­”ï¼ˆè«‹ä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼‰ï¼š
    """
    try:
        model = genai.GenerativeModel(target_model)
        response = model.generate_content(prompt)
        return response.text
    except Exception as e:
        return f"Gemini API ç”¢ç”Ÿç­”æ¡ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}"

# --- åˆå§‹åŒ–æµç¨‹ ---

st.set_page_config(page_title="æ”¾é›»æ©Ÿ AI åŠ©ç†", page_icon="âš¡")
st.title("âš¡ æ”¾é›»æ©Ÿæ“ä½œ AI å°å¹«æ‰‹")
st.caption("ä½¿ç”¨ RAG æŠ€è¡“èˆ‡ Gemini 2.0 Flash æ¨¡å‹ï¼Œå°ˆæ³¨æ–¼æä¾›å®‰å…¨ã€ç²¾æº–çš„æ‰‹å†Šè§£ç­”ã€‚")

if 'initialized' not in st.session_state:
    with st.spinner("ç³»çµ±æ­£åœ¨åˆ†ææ–‡ä»¶ä¸¦å»ºç«‹ç´¢å¼•ï¼Œè«‹ç¨å€™..."):
        # 1. è¼‰å…¥èªæ„æ¨¡å‹
        st.session_state.model = load_embedding_model()
        
        # 2. è®€å– .docx æª”æ¡ˆ
        current_folder = os.path.dirname(os.path.abspath(__file__))
        doc_names, doc_texts = load_documents(current_folder)
        
        if not doc_texts:
            st.error("âš ï¸ æ‰¾ä¸åˆ° .docx æª”æ¡ˆï¼è«‹ç¢ºä¿å·²å°‡ .doc å¦å­˜æ–°æª”ç‚º .docx ä¸¦ä¸Šå‚³è‡³ GitHubã€‚")
            st.stop()
            
        # 3. å»ºç«‹å‘é‡è³‡æ–™åº«
        chunks, _ = split_text(doc_names, doc_texts)
        embeddings = st.session_state.model.encode(chunks, show_progress_bar=False)
        st.session_state.chunks = chunks
        st.session_state.faiss_index = create_faiss_index(np.array(embeddings))
        
        st.session_state.initialized = True
        st.success(f"âœ… åˆå§‹åŒ–å®Œæˆï¼å·²è¼‰å…¥ {len(doc_names)} ä»½æ‰‹å†Šã€‚")

# --- ä½¿ç”¨è€…ä»‹é¢ ---

st.divider()
query = st.text_input("è«‹è¼¸å…¥æ‚¨çš„æ“ä½œå•é¡Œï¼š", placeholder="ä¾‹å¦‚ï¼šå¦‚ä½•é€²è¡Œå·¥ä»¶å°‹é‚Šï¼Ÿ")

if st.button("è©¢å• AI"):
    if query:
        with st.spinner("æ­£åœ¨æœå°‹ç­”æ¡ˆ..."):
            # å‘é‡æª¢ç´¢
            query_embedding = st.session_state.model.encode([query])
            distances, indices = st.session_state.faiss_index.search(query_embedding, 5)
            
            # ç²å–æœ€ç›¸é—œçš„æ®µè½
            retrieved_chunks = [st.session_state.chunks[i] for i in indices[0] if i != -1]
            context = "\n\n".join(retrieved_chunks)
            
            # ç”Ÿæˆç­”æ¡ˆ
            answer = generate_answer(query, context)
            
            st.markdown("### ğŸ¤– AI çš„å›ç­”ï¼š")
            st.success(answer)
            
            with st.expander("ğŸ” æŸ¥çœ‹åƒè€ƒä¾†æºæ®µè½"):
                for idx, chunk in enumerate(retrieved_chunks):
                    st.info(f"ä¾†æºç‰‡æ®µ {idx+1}:\n{chunk}")
    else:
        st.warning("è«‹å…ˆè¼¸å…¥å•é¡Œå…§å®¹ã€‚")