import streamlit as st
import os
import docx
from sentence_transformers import SentenceTransformer
import numpy as np
import faiss
import google.generativeai as genai

# ==========================================
# 1. é€™è£¡ç›´æ¥å¯«æ­»æ‚¨çš„ API KEY
# ==========================================
MY_API_KEY = "AIzaSyBVF_HR40eAuH_MmevkgWe5E33Ielm0eCw" 

# è¨­å®š Google Gemini
genai.configure(api_key=MY_API_KEY)

# å‘é‡æ¨¡å‹åç¨±
EMBEDDING_MODEL_NAME = 'paraphrase-multilingual-MiniLM-L12-v2'

# --- æ ¸å¿ƒåŠŸèƒ½å‡½å¼ ---

@st.cache_resource
def load_embedding_model():
    """è¼‰å…¥èªæ„åˆ†ææ¨¡å‹"""
    return SentenceTransformer(EMBEDDING_MODEL_NAME)

@st.cache_resource
def create_faiss_index(_embeddings):
    """å»ºç«‹æœå°‹ç´¢å¼•"""
    d = _embeddings.shape[1]
    index = faiss.IndexFlatL2(d)
    index.add(_embeddings)
    return index

def load_documents(folder_path):
    """è®€å–ç•¶å‰ç›®éŒ„ä¸‹çš„ Word æ–‡ä»¶"""
    doc_texts, doc_names = [], []
    files = [f for f in os.listdir(folder_path) if (f.endswith('.docx') or f.endswith('.doc')) and not f.startswith('~$')]
    for filename in files:
        try:
            full_path = os.path.join(folder_path, filename)
            doc = docx.Document(full_path)
            full_text = "\n\n".join([para.text for para in doc.paragraphs if para.text.strip()])
            doc_texts.append(full_text)
            doc_names.append(filename)
        except Exception as e:
            st.error(f"è®€å– {filename} å‡ºéŒ¯: {e}")
    return doc_names, doc_texts

def split_text(doc_names, doc_texts):
    """å°‡é•·æ–‡æœ¬åˆ‡å‰²æˆæ®µè½"""
    chunks, chunk_sources = [], []
    for i, text in enumerate(doc_texts):
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        chunks.extend(paragraphs)
        chunk_sources.extend([doc_names[i]] * len(paragraphs))
    return chunks, chunk_sources

def generate_answer(query, context):
    """èª¿ç”¨ Gemini ç”Ÿæˆç­”æ¡ˆ"""
    prompt = f"""
    ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ”¾é›»æ©Ÿ (EDM) æ“ä½œåŠ©æ‰‹ã€‚
    è«‹åƒ…æ ¹æ“šä»¥ä¸‹æ‰‹å†Šå…§å®¹å›ç­”å•é¡Œã€‚å¦‚æœæ‰‹å†Šæ²’æåˆ°ï¼Œè«‹èªªä¸çŸ¥é“ã€‚
    
    --- æ‰‹å†Šå…§å®¹ ---
    {context}
    ---
    å•é¡Œï¼š{query}
    å›ç­”ï¼ˆç¹é«”ä¸­æ–‡ï¼‰ï¼š
    """
    # é€™è£¡ç›´æ¥ä½¿ç”¨ 1.5-flashï¼Œç¢ºä¿ä¸æœƒæœ‰ 404 éŒ¯èª¤
    model = genai.GenerativeModel("gemini-1.5-flash")
    response = model.generate_content(prompt)
    return response.text

# --- åˆå§‹åŒ–æµç¨‹ ---
if 'initialized' not in st.session_state:
    with st.spinner("ç³»çµ±åˆå§‹åŒ–ä¸­ï¼Œè«‹ç¨å€™..."):
        # 1. è¼‰å…¥æ¨¡å‹
        st.session_state.model = load_embedding_model()
        # 2. è®€å–æ–‡ä»¶ (app.py æ‰€åœ¨ç›®éŒ„)
        current_folder = os.path.dirname(os.path.abspath(__file__))
        doc_names, doc_texts = load_documents(current_folder)
        
        if not doc_texts:
            st.error("éŒ¯èª¤ï¼šæ‰¾ä¸åˆ° .docx æ‰‹å†Šæª”æ¡ˆï¼")
            st.stop()
            
        # 3. å»ºç«‹å‘é‡è³‡æ–™åº«
        chunks, _ = split_text(doc_names, doc_texts)
        embeddings = st.session_state.model.encode(chunks)
        st.session_state.chunks = chunks
        st.session_state.faiss_index = create_faiss_index(np.array(embeddings))
        st.session_state.initialized = True

# --- ä½¿ç”¨è€…ä»‹é¢ ---
st.set_page_config(page_title="æ”¾é›»æ©Ÿ AI åŠ©ç†")
st.title("âš¡ æ”¾é›»æ©Ÿæ“ä½œ AI å°å¹«æ‰‹")
st.info("æœ¬ç³»çµ±å·²å…§å»º AI æˆæ¬Šï¼Œç›´æ¥è¼¸å…¥å•é¡Œå³å¯ã€‚")

query = st.text_input("è«‹è¼¸å…¥æ‚¨çš„æ“ä½œå•é¡Œï¼š", placeholder="ä¾‹å¦‚ï¼šå¦‚ä½•é€²è¡Œå°‹é‚Šæ“ä½œï¼Ÿ")

if st.button("è©¢å• AI"):
    if query:
        with st.spinner("æœå°‹æ‰‹å†Šå…§å®¹ä¸¦åˆ†æä¸­..."):
            # æª¢ç´¢æœ€ç›¸é—œçš„ 5 å€‹ç‰‡æ®µ
            query_embedding = st.session_state.model.encode([query])
            distances, indices = st.session_state.faiss_index.search(query_embedding, 5)
            
            context = "\n\n".join([st.session_state.chunks[i] for i in indices[0] if i != -1])
            
            # ç”Ÿæˆç­”æ¡ˆ
            answer = generate_answer(query, context)
            
            st.markdown("### ğŸ¤– å›ç­”çµæœï¼š")
            st.success(answer)
            
            with st.expander("æŸ¥çœ‹åƒè€ƒä¾†æºæ®µè½"):
                st.write(context)
    else:
        st.warning("è«‹å…ˆè¼¸å…¥å•é¡Œã€‚")